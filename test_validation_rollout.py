#!/usr/bin/env python3
"""
æµ‹è¯•å®Œæ•´è½¨è¿¹çš„è§†é¢‘ç”Ÿæˆï¼ˆå‚è€ƒrollout_replay_traj.pyï¼‰
æ»šåŠ¨é¢„æµ‹æ•´ä¸ªepisodeï¼Œç”Ÿæˆå®Œæ•´çš„å¯¹æ¯”è§†é¢‘
"""

import sys
sys.path.append('.')

import torch
import os
import numpy as np
from pathlib import Path
from config_flexiv import wm_args
import json
import einops
import mediapy
from tqdm import tqdm
from models.ctrl_world import CrtlWorld
from models.pipeline_ctrl_world import CtrlWorldDiffusionPipeline

print("="*60)
print("æµ‹è¯•å®Œæ•´è½¨è¿¹è§†é¢‘ç”Ÿæˆ")
print("="*60)

args = wm_args()
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 1. åŠ è½½æ¨¡å‹
print("\n1ï¸âƒ£ åŠ è½½æ¨¡å‹...")
try:
    model = CrtlWorld(args).to(device)
    
    if os.path.exists(args.ckpt_path):
        print(f"   åŠ è½½checkpoint: {args.ckpt_path}")
        state_dict = torch.load(args.ckpt_path, map_location='cpu')
        model.load_state_dict(state_dict, strict=False)
        print("   âœ“ CheckpointåŠ è½½å®Œæˆ")
    else:
        print(f"   âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆæµ‹è¯•ç”¨ï¼‰")
    
    model.eval()
    pipeline = model.pipeline
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 2. åŠ è½½æ•°æ®ç»Ÿè®¡ä¿¡æ¯
print("\n2ï¸âƒ£ åŠ è½½æ•°æ®ç»Ÿè®¡ä¿¡æ¯...")
with open(args.data_stat_path, 'r') as f:
    data_stat = json.load(f)
    state_p01 = np.array(data_stat['state_01'])[None, :]
    state_p99 = np.array(data_stat['state_99'])[None, :]
print(f"âœ… æ•°æ®ç»Ÿè®¡åŠ è½½æˆåŠŸ")

def normalize_bound(data, data_min, data_max, clip_min=-1, clip_max=1, eps=1e-8):
    ndata = 2 * (data - data_min) / (data_max - data_min + eps) - 1
    return np.clip(ndata, clip_min, clip_max)

# 3. è¯»å–éªŒè¯é›†çš„ä¸€ä¸ªepisode
print("\n3ï¸âƒ£ è¯»å–éªŒè¯episode...")
val_dataset_dir = args.val_dataset_dir

# è·å–éªŒè¯é›†ä¸­çš„æ‰€æœ‰episode
val_annotation_dir = Path(f"{val_dataset_dir}/annotation/val")
val_episodes = sorted([f.stem for f in val_annotation_dir.glob("*.json")])
print(f"   å¯ç”¨çš„éªŒè¯é›†episodes: {val_episodes}")

# ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„éªŒè¯episode
val_id = val_episodes[0]
start_idx = 0  # ä»ç¬¬0å¸§å¼€å§‹

annotation_path = f"{val_dataset_dir}/annotation/val/{val_id}.json"
print(f"   é€‰æ‹©Episode: {val_id}")
print(f"   Annotation: {annotation_path}")

with open(annotation_path) as f:
    anno = json.load(f)
    instruction = anno['texts'][0]
    
print(f"   Episode ID: {val_id}")
print(f"   Instruction: {instruction}")
print(f"   Start index: {start_idx}")

# 4. åŠ è½½å®Œæ•´çš„latent videos
print("\n4ï¸âƒ£ åŠ è½½latent videos...")
video_latents = []
for latent_info in anno['latent_videos']:
    latent_path = f"{val_dataset_dir}/{latent_info['latent_video_path']}"
    latent = torch.load(latent_path, map_location='cpu')
    video_latents.append(latent)
    print(f"   âœ“ {latent_path}: {latent.shape}")

print(f"âœ… åŠ è½½äº† {len(video_latents)} ä¸ªè§†è§’çš„latent")

# 5. åŠ è½½states/actions
print("\n5ï¸âƒ£ åŠ è½½stateså’Œactions...")
cartesian_pose = np.array(anno['observation.state.cartesian_position'])
gripper_pose = np.array(anno['observation.state.gripper_position'])

print(f"   Cartesian pose shape: {cartesian_pose.shape}")
print(f"   Gripper pose shape: {gripper_pose.shape}")

# æ‹¼æ¥æˆå®Œæ•´çš„state (cartesian + gripper)
if len(gripper_pose.shape) == 1:
    gripper_pose = gripper_pose[:, np.newaxis]  # (T,) -> (T, 1)
    
states = np.concatenate([cartesian_pose, gripper_pose], axis=-1)
print(f"   States shape (cartesian+gripper): {states.shape}")
print(f"   æ€»å¸§æ•°: {states.shape[0]}")

# 6. æ»šåŠ¨é¢„æµ‹è®¾ç½®
print("\n6ï¸âƒ£ é…ç½®æ»šåŠ¨é¢„æµ‹...")
pred_step = args.pred_step
num_history = args.num_history
num_frames = args.num_frames

# è®¡ç®—æœ€å¤§å¯é¢„æµ‹çš„äº¤äº’æ¬¡æ•°ï¼ˆé¿å…è¶…å‡ºepisodeé•¿åº¦ï¼‰
max_interact_num = (states.shape[0] - start_idx - num_history) // (pred_step - 1)

# é™åˆ¶ç”Ÿæˆæ—¶é•¿ï¼ˆé»˜è®¤20ç§’ï¼‰
target_duration = 20  # ç§’
target_frames = int(target_duration * args.fps)
target_interact_num = target_frames // (pred_step - 1)
interact_num = min(target_interact_num, max_interact_num)

total_frames = interact_num * (pred_step - 1)
duration_seconds = total_frames / args.fps

print(f"   pred_step: {pred_step}")
print(f"   num_history: {num_history}")
print(f"   num_frames: {num_frames}")
print(f"   ç›®æ ‡æ—¶é•¿: {target_duration} ç§’")
print(f"   interact_num: {interact_num} (æœ€å¤§{max_interact_num})")
print(f"   æ€»å…±é¢„æµ‹å¸§æ•°: {total_frames} å¸§")
print(f"   å®é™…è§†é¢‘æ—¶é•¿: {duration_seconds:.1f} ç§’")
print(f"   åŸå§‹episodeé•¿åº¦: {states.shape[0]} å¸§ ({states.shape[0]/args.fps:.1f} ç§’)")

# 7. å¼€å§‹æ»šåŠ¨é¢„æµ‹
print("\n7ï¸âƒ£ å¼€å§‹æ»šåŠ¨é¢„æµ‹...")

# åˆå§‹åŒ–history buffer
his_cond = []
his_states = []

# æ‹¼æ¥ç¬¬ä¸€å¸§çš„latentï¼ˆFlexiv: 2ä¸ªè§†è§’ â†’ å¤ç”¨ç¬¬3ä¸ªï¼‰
num_views = len(video_latents)
latent_list = [v[start_idx:start_idx+1] for v in video_latents]

# å¦‚æœåªæœ‰2ä¸ªè§†è§’ï¼Œå¤ç”¨ç¬¬2ä¸ªä½œä¸ºç¬¬3ä¸ª
if num_views == 2:
    latent_list.append(latent_list[1])  # å¤ç”¨ç¬¬2ä¸ªè§†è§’
    print(f"   æ£€æµ‹åˆ°{num_views}ä¸ªè§†è§’ï¼Œå¤ç”¨ç¬¬2ä¸ªä½œä¸ºç¬¬3ä¸ª")
    
first_latent = torch.cat(latent_list, dim=2).to(device)  # (1, 4, 72, 40)
print(f"   first_latent shape: {first_latent.shape}")
assert first_latent.shape[2] == 72, f"Expected height=72, got {first_latent.shape[2]}"

# å¡«å……history buffer
for i in range(num_history * 4):
    his_cond.append(first_latent)
    his_states.append(states[start_idx:start_idx+1])

# å­˜å‚¨ç»“æœ
video_to_save = []

# æ»šåŠ¨é¢„æµ‹å¾ªç¯
for i in tqdm(range(interact_num), desc="æ»šåŠ¨é¢„æµ‹", unit="æ­¥"):
    if i % 10 == 0:  # æ¯10æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
        print(f"\n   ===== é¢„æµ‹æ­¥éª¤ {i+1}/{interact_num} =====")
    
    # å½“å‰æ­¥éª¤çš„å¸§èŒƒå›´
    step_start = start_idx + int(i * (pred_step - 1))
    step_end = step_start + pred_step
    
    if i % 10 == 0:
        print(f"   å¸§èŒƒå›´: {step_start} ~ {step_end}")
    
    # å‡†å¤‡ground truth latentsï¼ˆ2ä¸ªè§†è§’ â†’ å¤ç”¨ç¬¬3ä¸ªï¼‰
    video_latent_true = [v[step_start:step_end].to(device) for v in video_latents]
    if len(video_latent_true) == 2:
        video_latent_true.append(video_latent_true[1])  # å¤ç”¨ç¬¬2ä¸ªè§†è§’
    
    # å‡†å¤‡action condition
    history_idx = [0, 0, -8, -6, -4, -2]
    his_pose = np.concatenate([his_states[idx] for idx in history_idx], axis=0)
    action_seq = states[step_start:step_end]
    action_cond = np.concatenate([his_pose, action_seq], axis=0)
    
    # å½’ä¸€åŒ–action
    action_cond = normalize_bound(action_cond, state_p01, state_p99)
    action_cond = torch.tensor(action_cond).unsqueeze(0).to(device).to(torch.float32)
    
    # å‡†å¤‡historyå’Œcurrent latent
    his_cond_input = torch.cat([his_cond[idx] for idx in history_idx], dim=0).unsqueeze(0)
    current_latent = his_cond[-1]
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        # Encode action
        action_latent = model.action_encoder(
            action_cond, [instruction], model.tokenizer, model.text_encoder, args.frame_level_cond
        )
        
        # Generate video
        _, pred_latents = CtrlWorldDiffusionPipeline.__call__(
            pipeline,
            image=current_latent,
            text=action_latent,
            width=args.width,
            height=int(3*args.height),
            num_frames=args.num_frames,
            history=his_cond_input,
            num_inference_steps=args.num_inference_steps,
            decode_chunk_size=args.decode_chunk_size,
            max_guidance_scale=args.guidance_scale,
            fps=args.fps,
            motion_bucket_id=args.motion_bucket_id,
            mask=None,
            output_type='latent',
            return_dict=False,
            frame_level_cond=args.frame_level_cond,
            his_cond_zero=args.his_cond_zero,
        )
    
    # Rearrange
    pred_latents = einops.rearrange(pred_latents, 'b f c (m h) (n w) -> (b m n) f c h w', m=3, n=1)
    
    # Decode ground truth
    true_video = torch.stack(video_latent_true, dim=0)  # (3, T, 4, 24, 40)
    decoded_true = []
    bsz, frame_num = true_video.shape[:2]
    true_video_flat = true_video.flatten(0, 1)
    
    decode_kwargs = {}
    for j in range(0, true_video_flat.shape[0], args.decode_chunk_size):
        chunk = true_video_flat[j:j+args.decode_chunk_size] / pipeline.vae.config.scaling_factor
        decode_kwargs["num_frames"] = chunk.shape[0]
        decoded_true.append(pipeline.vae.decode(chunk, **decode_kwargs).sample)
    
    true_video = torch.cat(decoded_true, dim=0)
    true_video = true_video.reshape(bsz, frame_num, *true_video.shape[1:])
    true_video = ((true_video / 2.0 + 0.5).clamp(0, 1) * 255)
    true_video = true_video.detach().to(torch.float32).cpu().numpy().transpose(0, 1, 3, 4, 2).astype(np.uint8)
    
    # Decode prediction
    decoded_pred = []
    bsz, frame_num = pred_latents.shape[:2]
    pred_latents_flat = pred_latents.flatten(0, 1)
    
    for j in range(0, pred_latents_flat.shape[0], args.decode_chunk_size):
        chunk = pred_latents_flat[j:j+args.decode_chunk_size] / pipeline.vae.config.scaling_factor
        decode_kwargs["num_frames"] = chunk.shape[0]
        decoded_pred.append(pipeline.vae.decode(chunk, **decode_kwargs).sample)
    
    pred_video = torch.cat(decoded_pred, dim=0)
    pred_video = pred_video.reshape(bsz, frame_num, *pred_video.shape[1:])
    pred_video = ((pred_video / 2.0 + 0.5).clamp(0, 1) * 255)
    pred_video = pred_video.detach().to(torch.float32).cpu().numpy().transpose(0, 1, 3, 4, 2).astype(np.uint8)
    
    # æ‹¼æ¥GTå’Œé¢„æµ‹
    videos_cat = np.concatenate([true_video, pred_video], axis=-3)  # (3, T, H*2, W, 3)
    videos_cat = np.concatenate([video for video in videos_cat], axis=-2).astype(np.uint8)  # (T, H*2, W*3, 3)
    
    # ä¿å­˜åˆ°bufferï¼ˆå»æ‰æœ€åä¸€å¸§é¿å…é‡å¤ï¼‰
    if i == interact_num - 1:
        video_to_save.append(videos_cat)
    else:
        video_to_save.append(videos_cat[:pred_step-1])
    
    # æ›´æ–°historyï¼ˆæ‹¼æ¥3ä¸ªè§†è§’çš„é¢„æµ‹ç»“æœï¼‰
    his_states.append(action_seq[pred_step-1:pred_step])
    # pred_latentså·²ç»æ˜¯3ä¸ªè§†è§’äº†ï¼ˆrearrangeåæ˜¯6ä¸ªï¼šbatch0-view0/1/2, batch1-view0/1/2ï¼‰
    # å¯¹äºbatch 0ï¼Œå–å‰3ä¸ªè§†è§’
    pred_last_latent = torch.cat([pred_latents[j, pred_step-1:pred_step] for j in range(3)], dim=2)  # (1, 4, 72, 40)
    his_cond.append(pred_last_latent)

# 8. æ‹¼æ¥å¹¶ä¿å­˜å®Œæ•´è§†é¢‘
print("\n8ï¸âƒ£ ä¿å­˜å®Œæ•´è½¨è¿¹è§†é¢‘...")
output_dir = Path("test_validation_output")
output_dir.mkdir(exist_ok=True)

video_full = np.concatenate(video_to_save, axis=0)
print(f"   å®Œæ•´è§†é¢‘shape: {video_full.shape}")

output_path = output_dir / f"test_rollout_val{val_id}_start{start_idx}.mp4"
mediapy.write_video(str(output_path), video_full, fps=args.fps)

print(f"âœ… å®Œæ•´è½¨è¿¹è§†é¢‘ä¿å­˜: {output_path}")
print(f"   æ€»å¸§æ•°: {video_full.shape[0]}")
print(f"   åˆ†è¾¨ç‡: {video_full.shape[1]}Ã—{video_full.shape[2]}")
print(f"   å¸ƒå±€: GTä¸Š + é¢„æµ‹ä¸‹ + 3è§†è§’æ¨ªæ’")

print("\n" + "="*60)
print("ğŸ‰ å®Œæ•´è½¨è¿¹è§†é¢‘ç”Ÿæˆå®Œæˆï¼")
print("="*60)

