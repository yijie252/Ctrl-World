#!/usr/bin/env python3
"""
æµ‹è¯•éªŒè¯è§†é¢‘ç”Ÿæˆçš„å®Œæ•´æµç¨‹
"""

import sys
sys.path.append('.')

import torch
import os
import numpy as np
from pathlib import Path
from config_flexiv import wm_args

print("="*60)
print("æµ‹è¯•éªŒè¯è§†é¢‘ç”Ÿæˆå®Œæ•´æµç¨‹")
print("="*60)

args = wm_args()

# 1. åŠ è½½éªŒè¯æ•°æ®é›†
print("\n1ï¸âƒ£ åŠ è½½éªŒè¯æ•°æ®é›†...")
try:
    from dataset.dataset_droid_exp33 import Dataset_mix
    val_dataset = Dataset_mix(args, mode='val')
    print(f"âœ… éªŒè¯é›†åŠ è½½æˆåŠŸ: {len(val_dataset)} samples")
except Exception as e:
    print(f"âŒ åŠ è½½éªŒè¯é›†å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 2. æ¨¡æ‹ŸéªŒè¯batché‡‡æ ·ï¼ˆå’Œtrain_wm.pyä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
print("\n2ï¸âƒ£ æ¨¡æ‹ŸéªŒè¯batché‡‡æ ·...")
try:
    videos_row = args.video_num if not args.debug else 1
    videos_col = 2
    id = 0  # ç¬¬0ä¸ªGPU
    
    # é‡‡æ ·é€»è¾‘å’Œvalidate_video_generationå‡½æ•°ä¸­ä¸€è‡´
    batch_id = list(range(0, len(val_dataset), int(len(val_dataset)/videos_row/videos_col)))
    batch_id = batch_id[int(id*(videos_col)):int((id+1)*(videos_col))]
    
    print(f"   videos_row: {videos_row}")
    print(f"   videos_col: {videos_col}")
    print(f"   batch_id: {batch_id}")
    print(f"   é‡‡æ · {len(batch_id)} ä¸ªæ ·æœ¬")
    
    batch_list = [val_dataset.__getitem__(bid) for bid in batch_id]
    print(f"âœ… Batché‡‡æ ·æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ Batché‡‡æ ·å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 3. æ„é€ batchæ•°æ®ï¼ˆæ¨¡æ‹ŸGPU tensorï¼‰
print("\n3ï¸âƒ£ æ„é€ batchæ•°æ®...")
try:
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
    
    video_gt = torch.cat([t['latent'].unsqueeze(0) for i,t in enumerate(batch_list)], dim=0)
    text = [t['text'] for i,t in enumerate(batch_list)]
    actions = torch.cat([t['action'].unsqueeze(0) for i,t in enumerate(batch_list)], dim=0)
    
    # ç§»åˆ°è®¾å¤‡ä¸Šï¼ˆå¦‚æœæœ‰GPUï¼‰
    video_gt = video_gt.to(device)
    actions = actions.to(device)
    
    print(f"   video_gt shape: {video_gt.shape}")
    print(f"   actions shape: {actions.shape}")
    print(f"   text samples: {len(text)}")
    
    print(f"âœ… Batchæ•°æ®æ„é€ æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ Batchæ•°æ®æ„é€ å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 4. æµ‹è¯•éªŒè¯ä»£ç ä¸­çš„assertioné€»è¾‘
print("\n4ï¸âƒ£ æµ‹è¯•assertioné€»è¾‘...")
try:
    his_latent_gt, future_latent_ft = video_gt[:,:args.num_history], video_gt[:,args.num_history:]
    current_latent = future_latent_ft[:,0]
    
    print(f"   his_latent_gt shape: {his_latent_gt.shape}")
    print(f"   future_latent_ft shape: {future_latent_ft.shape}")
    print(f"   current_latent shape: {current_latent.shape}")
    print(f"   actions shape: {actions.shape}")
    
    # æ‰§è¡Œå’Œtrain_wm.pyä¸­å®Œå…¨ç›¸åŒçš„assertion
    print("\n   æ‰§è¡Œassertionæ£€æŸ¥...")
    
    # æ£€æŸ¥1: channels
    assert current_latent.shape[1] == 4, f"Expected 4 channels, got {current_latent.shape[1]}"
    print(f"   âœ“ Channels: {current_latent.shape[1]} == 4")
    
    # æ£€æŸ¥2: width
    expected_latent_width = args.width // 8  # VAEä¸‹é‡‡æ ·8å€: 320//8=40
    assert current_latent.shape[3] == expected_latent_width, f"Expected width {expected_latent_width}, got {current_latent.shape[3]}"
    print(f"   âœ“ Width: {current_latent.shape[3]} == {expected_latent_width}")
    
    # æ£€æŸ¥3: heightï¼ˆå¤šè§†è§’ï¼‰
    num_views = current_latent.shape[2] // 24
    print(f"   âœ“ Height: {current_latent.shape[2]} = {num_views} views Ã— 24")
    
    # æ£€æŸ¥4: actions
    assert actions.shape[1:] == (int(args.num_frames+args.num_history), args.action_dim), \
        f"Expected actions shape {(int(args.num_frames+args.num_history), args.action_dim)}, got {actions.shape[1:]}"
    print(f"   âœ“ Actions: {actions.shape[1:]} == ({args.num_frames+args.num_history}, {args.action_dim})")
    
    print(f"\nâœ… æ‰€æœ‰assertionæ£€æŸ¥é€šè¿‡ï¼")
    
except AssertionError as e:
    print(f"âŒ Assertionå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 5. æµ‹è¯•einops rearrangeæ“ä½œ
print("\n5ï¸âƒ£ æµ‹è¯•einops rearrangeæ“ä½œ...")
try:
    import einops
    
    # æ¨¡æ‹Ÿpred_latents
    B, F = current_latent.shape[0], args.num_frames
    C, H, W = current_latent.shape[1], current_latent.shape[2], current_latent.shape[3]
    
    dummy_pred_latents = torch.randn(B, F, C, H, W, device=device)
    print(f"   dummy_pred_latents shape: {dummy_pred_latents.shape}")
    
    # æ‰§è¡Œrearrangeï¼ˆéªŒè¯ä»£ç ä¸­çš„æ“ä½œï¼‰
    rearranged = einops.rearrange(dummy_pred_latents, 'b f c (m h) (n w) -> (b m n) f c h w', m=3, n=1)
    print(f"   rearranged shape: {rearranged.shape}")
    print(f"   æœŸæœ›: ({B}*3*1, {F}, {C}, {H//3}, {W}) = ({B*3}, {F}, {C}, {H//3}, {W})")
    
    # å¯¹video_gtä¹Ÿæ‰§è¡Œrearrange
    video_gt_rearranged = einops.rearrange(video_gt, 'b f c (m h) (n w) -> (b m n) f c h w', m=3, n=1)
    print(f"   video_gt_rearranged shape: {video_gt_rearranged.shape}")
    
    print(f"âœ… einops rearrangeæ“ä½œæˆåŠŸ")
    
except Exception as e:
    print(f"âŒ einops rearrangeå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 6. æµ‹è¯•VAE decodeå‚æ•°
print("\n6ï¸âƒ£ æµ‹è¯•VAE decodeå‚æ•°...")
try:
    # æ£€æŸ¥heightå‚æ•°
    expected_height = int(3 * args.height)
    print(f"   args.height: {args.height}")
    print(f"   ç”Ÿæˆæ—¶çš„heightå‚æ•°: 3 Ã— {args.height} = {expected_height}")
    print(f"   å®é™…latentçš„height: {current_latent.shape[2]}")
    
    # æ£€æŸ¥decode_chunk_size
    print(f"   decode_chunk_size: {args.decode_chunk_size}")
    
    # æ¨¡æ‹Ÿåˆ†chunk decode
    num_chunks = (video_gt.shape[0] * video_gt.shape[1] + args.decode_chunk_size - 1) // args.decode_chunk_size
    print(f"   æ€»å¸§æ•°: {video_gt.shape[0]} Ã— {video_gt.shape[1]} = {video_gt.shape[0] * video_gt.shape[1]}")
    print(f"   éœ€è¦ {num_chunks} ä¸ªchunksæ¥decode")
    
    print(f"âœ… VAE decodeå‚æ•°æ£€æŸ¥é€šè¿‡")
    
except Exception as e:
    print(f"âŒ VAE decodeå‚æ•°æ£€æŸ¥å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 7. åŠ è½½æ¨¡å‹å¹¶ç”Ÿæˆè§†é¢‘
print("\n7ï¸âƒ£ åŠ è½½æ¨¡å‹å¹¶ç”Ÿæˆè§†é¢‘...")
try:
    from models.ctrl_world import CrtlWorld
    from models.pipeline_ctrl_world import CtrlWorldDiffusionPipeline
    
    print("   åˆå§‹åŒ–æ¨¡å‹...")
    model = CrtlWorld(args).to(device)
    
    # åŠ è½½checkpoint
    if os.path.exists(args.ckpt_path):
        print(f"   åŠ è½½checkpoint: {args.ckpt_path}")
        state_dict = torch.load(args.ckpt_path, map_location='cpu')
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        if missing_keys:
            print(f"   âš ï¸  Missing keys: {len(missing_keys)}")
        if unexpected_keys:
            print(f"   âš ï¸  Unexpected keys: {len(unexpected_keys)}")
        print("   âœ“ CheckpointåŠ è½½å®Œæˆ")
    else:
        print(f"   âš ï¸  Checkpointä¸å­˜åœ¨: {args.ckpt_path}")
        print("   ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰")
    
    model.eval()
    pipeline = model.pipeline
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 8. æ‰§è¡Œè§†é¢‘ç”Ÿæˆ
print("\n8ï¸âƒ£ æ‰§è¡Œè§†é¢‘ç”Ÿæˆ...")
try:
    with torch.no_grad():
        print("   ç¼–ç action...")
        bsz = actions.shape[0]
        action_latent = model.action_encoder(
            actions, text, model.tokenizer, model.text_encoder, args.frame_level_cond
        )
        print(f"   action_latent shape: {action_latent.shape}")
        
        print("   ç”Ÿæˆè§†é¢‘latent...")
        _, pred_latents = CtrlWorldDiffusionPipeline.__call__(
            pipeline,
            image=current_latent,
            text=action_latent,
            width=args.width,
            height=int(3*args.height),
            num_frames=args.num_frames,
            history=his_latent_gt,
            num_inference_steps=args.num_inference_steps,
            decode_chunk_size=args.decode_chunk_size,
            max_guidance_scale=args.guidance_scale,
            fps=args.fps,
            motion_bucket_id=args.motion_bucket_id,
            mask=None,
            output_type='latent',
            return_dict=False,
            frame_level_cond=args.frame_level_cond,
            his_cond_zero=args.his_cond_zero,
        )
        print(f"   pred_latents shape: {pred_latents.shape}")
    
    print(f"âœ… è§†é¢‘latentç”ŸæˆæˆåŠŸ")
    
except Exception as e:
    print(f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 9. Rearrangeå’Œdecodeï¼ˆå‚è€ƒrollout_replay_traj.pyçš„å®Œæ•´æµç¨‹ï¼‰
print("\n9ï¸âƒ£ Rearrangeå’Œdecodeè§†é¢‘...")
try:
    import einops
    
    # Rearrange pred_latents
    print("   Rearrange pred_latents...")
    pred_latents_rearranged = einops.rearrange(
        pred_latents, 'b f c (m h) (n w) -> (b m n) f c h w', m=3, n=1
    )
    print(f"   pred_latents_rearranged shape: {pred_latents_rearranged.shape}")
    
    # Prepare ground truth video (åªç”¨futureéƒ¨åˆ†ï¼Œå’Œé¢„æµ‹å¸§æ•°åŒ¹é…)
    print("   Prepare ground truth video (future frames only)...")
    # åªå–future_latent_ftï¼Œä¸åŒ…å«historyï¼Œè¿™æ ·å’Œé¢„æµ‹çš„å¸§æ•°ä¸€è‡´
    video_gt_future = future_latent_ft  # åªç”¨futureéƒ¨åˆ†
    video_gt_rearranged = einops.rearrange(
        video_gt_future, 'b f c (m h) (n w) -> (b m n) f c h w', m=3, n=1
    )
    print(f"   video_gt_rearranged shape: {video_gt_rearranged.shape}")
    
    # Decode ground truth video
    print("   Decode ground truth video...")
    true_video = video_gt_rearranged
    decoded_true = []
    bsz_true, frame_num_true = true_video.shape[:2]
    true_video_flat = true_video.flatten(0, 1)
    decode_kwargs = {}
    
    for i in range(0, true_video_flat.shape[0], args.decode_chunk_size):
        chunk = true_video_flat[i:i+args.decode_chunk_size] / pipeline.vae.config.scaling_factor
        decode_kwargs["num_frames"] = chunk.shape[0]
        decoded_true.append(pipeline.vae.decode(chunk, **decode_kwargs).sample)
    
    true_video_decoded = torch.cat(decoded_true, dim=0)
    true_video_decoded = true_video_decoded.reshape(bsz_true, frame_num_true, *true_video_decoded.shape[1:])
    
    # è½¬æ¢ä¸ºnumpyæ ¼å¼: (bsz, T, C, H, W) -> (bsz, T, H, W, C)
    true_video_np = ((true_video_decoded / 2.0 + 0.5).clamp(0, 1) * 255)
    true_video_np = true_video_np.detach().to(torch.float32).cpu().numpy().transpose(0, 1, 3, 4, 2).astype(np.uint8)
    print(f"   true_video_np shape: {true_video_np.shape}")
    
    # Decode predicted video
    print("   Decode predicted video...")
    decoded_pred = []
    bsz_pred, frame_num_pred = pred_latents_rearranged.shape[:2]
    pred_latents_flat = pred_latents_rearranged.flatten(0, 1)
    
    for i in range(0, pred_latents_flat.shape[0], args.decode_chunk_size):
        chunk = pred_latents_flat[i:i+args.decode_chunk_size] / pipeline.vae.config.scaling_factor
        decode_kwargs["num_frames"] = chunk.shape[0]
        decoded_pred.append(pipeline.vae.decode(chunk, **decode_kwargs).sample)
    
    pred_video_decoded = torch.cat(decoded_pred, dim=0)
    pred_video_decoded = pred_video_decoded.reshape(bsz_pred, frame_num_pred, *pred_video_decoded.shape[1:])
    
    # è½¬æ¢ä¸ºnumpyæ ¼å¼
    pred_video_np = ((pred_video_decoded / 2.0 + 0.5).clamp(0, 1) * 255)
    pred_video_np = pred_video_np.detach().to(torch.float32).cpu().numpy().transpose(0, 1, 3, 4, 2).astype(np.uint8)
    print(f"   pred_video_np shape: {pred_video_np.shape}")
    
    print(f"âœ… è§†é¢‘decodeæˆåŠŸ")
    
except Exception as e:
    print(f"âŒ è§†é¢‘decodeå¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 10. ä¿å­˜å¯¹æ¯”è§†é¢‘ï¼ˆGround Truth vs Predictedï¼Œå‚è€ƒrollout_replay_traj.pyï¼‰
print("\nğŸ”Ÿ ä¿å­˜å¯¹æ¯”è§†é¢‘...")
try:
    import mediapy
    import numpy as np
    from torchvision.utils import save_image
    
    output_dir = Path("test_validation_output")
    output_dir.mkdir(exist_ok=True)
    
    print(f"   true_video_np shape: {true_video_np.shape}")
    print(f"   pred_video_np shape: {pred_video_np.shape}")
    
    # æ‹¼æ¥GTå’Œé¢„æµ‹è§†é¢‘ï¼Œå®Œå…¨æŒ‰ç…§rollout_replay_traj.pyçš„æ–¹å¼
    # true_video_np: (3, T, H, W, 3)
    # pred_video_np: (3, T, H, W, 3)
    
    # æ²¿ç€é«˜åº¦ç»´åº¦æ‹¼æ¥GTå’Œé¢„æµ‹: (3, T, H, W, 3) -> (3, T, H*2, W, 3)
    videos_cat = np.concatenate([true_video_np, pred_video_np], axis=-3)
    print(f"   videos_cat shape after concat: {videos_cat.shape}")  # (3, T, H*2, W, 3)
    
    # æ²¿ç€å®½åº¦ç»´åº¦æ‹¼æ¥3ä¸ªè§†è§’: (3, T, H*2, W, 3) -> (T, H*2, W*3, 3)
    videos_cat = np.concatenate([video for video in videos_cat], axis=-2).astype(np.uint8)
    print(f"   videos_cat shape final: {videos_cat.shape}")  # (T, H*2, W*3, 3)
    
    # ä¿å­˜å®Œæ•´çš„å¯¹æ¯”è§†é¢‘
    mp4_path = output_dir / "test_validation_comparison.mp4"
    mediapy.write_video(str(mp4_path), videos_cat, fps=args.fps)
    print(f"   âœ“ ä¿å­˜å¯¹æ¯”è§†é¢‘: {mp4_path}")
    print(f"      å¸ƒå±€: ä¸ŠåŠéƒ¨åˆ†=Ground Truth (3è§†è§’æ¨ªæ’)")
    print(f"            ä¸‹åŠéƒ¨åˆ†=Predicted (3è§†è§’æ¨ªæ’)")
    print(f"      Shape: {videos_cat.shape} ({videos_cat.shape[0]} frames, {args.fps} fps)")
    
    # å¦å¤–ä¿å­˜å•ç‹¬çš„GTå’Œé¢„æµ‹è§†é¢‘
    print("\n   ä¿å­˜å•ç‹¬è§†é¢‘...")
    
    # GTè§†é¢‘ï¼ˆ3è§†è§’æ¨ªæ’ï¼‰
    true_video_concat = np.concatenate([video for video in true_video_np], axis=-2).astype(np.uint8)
    gt_path = output_dir / "test_ground_truth.mp4"
    mediapy.write_video(str(gt_path), true_video_concat, fps=args.fps)
    print(f"   âœ“ ä¿å­˜GTè§†é¢‘: {gt_path} ({true_video_concat.shape})")
    
    # é¢„æµ‹è§†é¢‘ï¼ˆ3è§†è§’æ¨ªæ’ï¼‰
    pred_video_concat = np.concatenate([video for video in pred_video_np], axis=-2).astype(np.uint8)
    pred_path = output_dir / "test_predicted.mp4"
    mediapy.write_video(str(pred_path), pred_video_concat, fps=args.fps)
    print(f"   âœ“ ä¿å­˜é¢„æµ‹è§†é¢‘: {pred_path} ({pred_video_concat.shape})")
    
    # ä¿å­˜æ¯ä¸ªè§†è§’çš„å•ç‹¬è§†é¢‘
    print("\n   ä¿å­˜å„è§†è§’å•ç‹¬è§†é¢‘...")
    for view_idx in range(3):
        # GTè§†è§’
        gt_view_path = output_dir / f"test_gt_view{view_idx}.mp4"
        mediapy.write_video(str(gt_view_path), true_video_np[view_idx], fps=args.fps)
        
        # é¢„æµ‹è§†è§’
        pred_view_path = output_dir / f"test_pred_view{view_idx}.mp4"
        mediapy.write_video(str(pred_view_path), pred_video_np[view_idx], fps=args.fps)
        
        print(f"   âœ“ è§†è§’{view_idx}: {gt_view_path.name} + {pred_view_path.name}")
    
    print(f"\nâœ… å¯¹æ¯”è§†é¢‘ä¿å­˜æˆåŠŸï¼")
    print(f"   ä¸»è¦æ–‡ä»¶: {mp4_path.name} (GTä¸Š + é¢„æµ‹ä¸‹ + 3è§†è§’æ¨ªæ’)")
    
except Exception as e:
    print(f"âš ï¸  ä¿å­˜è§†é¢‘å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")
    import traceback
    traceback.print_exc()

# æ€»ç»“
print("\n" + "="*60)
print("ğŸ“Š æµ‹è¯•æ€»ç»“")
print("="*60)
print(f"âœ… éªŒè¯é›†æ ·æœ¬æ•°: {len(val_dataset)}")
print(f"âœ… Batchå¤§å°: {len(batch_list)}")
print(f"âœ… Latentç»´åº¦: {current_latent.shape}")
print(f"âœ… Actionsç»´åº¦: {actions.shape}")
print(f"âœ… è§†è§’æ•°é‡: {num_views}")
print(f"âœ… æ‰€æœ‰assertionæ£€æŸ¥: é€šè¿‡")
print(f"âœ… einopsæ“ä½œ: é€šè¿‡")
print(f"âœ… VAEå‚æ•°: æ­£ç¡®")
print(f"âœ… æ¨¡å‹åŠ è½½: æˆåŠŸ")
print(f"âœ… è§†é¢‘ç”Ÿæˆ: æˆåŠŸ")
print(f"âœ… è§†é¢‘decode: æˆåŠŸ (GT + é¢„æµ‹)")
print(f"âœ… å¯¹æ¯”è§†é¢‘ä¿å­˜: æˆåŠŸ")
print("\nğŸ‰ éªŒè¯è§†é¢‘ç”Ÿæˆå®Œæ•´æµ‹è¯•é€šè¿‡ï¼")
print("   è®­ç»ƒä¸­çš„éªŒè¯æ­¥éª¤å®Œå…¨æ­£å¸¸ã€‚")
print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
print(f"   ä¸»è§†é¢‘: test_validation_output/test_validation_comparison.mp4")
print(f"   å¸ƒå±€: [GTä¸Š + é¢„æµ‹ä¸‹] Ã— [3è§†è§’æ¨ªæ’]")
print("="*60)

